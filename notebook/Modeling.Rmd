---
title: "Modeling"
author: "Karine Almeida"
date: "2023-10-12"
output: html_document
---

#Modeling

- In this notebook we will apply machine learning models, with the aim of predicting the price of used cars based on the relationship between the price and a set of resources.  
- Since we are dealing with a continuous variable, we will apply regression, which is part of supervised machine learning models.  
- We will apply 4 models and, through evaluation metrics, identify the one that performs best. This model will be used to obtain the price prediction from the *(cars_test)* dataset.  


## Viewing the database   
```{r echo=TRUE, warning=FALSE}
cars_train <- read_csv("../data/cars_train_clean1.csv", show_col_types = FALSE)
glimpse(cars_train)
``` 
Detect near zero covariates  
```{r}
nearZeroVar(cars_train, saveMetrics = T)
```

Removing variables near zero covariates and variables with excess categories (high computational power)  
```{r}
cars_train <- subset(cars_train, 
                     select = -c(num_portas, blindado, cidade_vendedor, modelo, versao))

```

Transforming variables into factors
```{r}
cars_train[,c("marca", "cor", "cambio", "tipo", "ano_de_fabricacao",
              "ano_modelo","veiculo_único_dono",
              "ipva_pago", "veiculo_licenciado",
              "tipo_vendedor","estado_vendedor", 
              "anunciante")]= 
  lapply(cars_train[,c("marca", "cor", "cambio", "tipo", "ano_de_fabricacao",
              "ano_modelo","veiculo_único_dono",
              "ipva_pago", "veiculo_licenciado",
              "tipo_vendedor","estado_vendedor", 
              "anunciante")], as.factor)

summary(cars_train)
```

Dummize qualitative variables before running the model 
```{r}
cars_train_dummy <- dummy_columns(.data = cars_train,
                             select_columns = 
                               c("marca","cor","cambio",
                                 "tipo","ano_de_fabricacao",
                                 "ano_modelo",
                                 "veiculo_único_dono",
                                 "ipva_pago", "veiculo_licenciado",
                                 "tipo_vendedor"
                                 ,"estado_vendedor","anunciante"),
                             remove_selected_columns = T,
                             remove_most_frequent_dummy = T)

summary(cars_train_dummy)
```

# Linear Models
Multiple Linear Regression
```{r}
modelo_cars_train <- lm(preco~., cars_train_dummy)
summary(modelo_cars_train)
```
  
Evaluating model
```{r}
predito_OLS <- predict(modelo_cars_train, cars_train_dummy)
metricas2(predito_OLS, cars_train_dummy$preco)
```
  
Multiple Nonlinear Regression (Box-Cox Transformation)
```{r}
lambda_BC <- powerTransform(cars_train_dummy$preco)
lambda_BC

#Inserindo o lambda de Box-Cox na base de dados para a estimação de um novo modelo
cars_train_dummy$bcpreco <- (((cars_train_dummy$preco ^ lambda_BC$lambda) - 1) / 
                                   lambda_BC$lambda)

#Estimando um novo modelo múltiplo com variável dependente transformada por Box-Cox 
modelo_cars_train_bc <- lm(formula = bcpreco ~ . -preco, na.rm = T,
                        data = cars_train_dummy)
summary(modelo_cars_train_bc)
```
    
Evaluating model  
```{r}
predito_bc <- predict(modelo_cars_train_bc, cars_train_dummy)
metricas2(predito_bc, cars_train_dummy$bcpreco)
```
  
# Tree regression

```{r include=FALSE}
cars_train2 <- read_csv("../data/cars_train_clean.csv", show_col_types = FALSE)

#Removing variables with excess categories
cars_train2 <- subset(cars_train2, 
                     select = -c(cidade_vendedor, modelo, versao))
```

Train tree
```{r}
tree <- rpart(preco ~ .,
                 data= cars_train2,
                 xval=5,
                 control = rpart.control(cp = 0, 
                                         minsplit = 2,
                                         maxdepth = 30))
```

Save the forecast
```{r}
cars_train2['predict'] = predict(tree, cars_train2)

# investigate the forecast
cars_train2$predict %>% tail
```

creating residue variable
```{r}
cars_train2['residue'] = cars_train2$preco - cars_train2$predict

#Investigate residue

cars_train2$residue %>% tail
```

Evaluate the tree
```{r}
metricas2(cars_train2$predict, cars_train2$preco)
```
```{r}
scatterplot_color(cars_train2, "predict", "preco", "residue")
```
```{r}
#analise_grafica(cars_train2, "anunciante", "preco", "predict", "residue")
```

Observe the complexity of tree paths (cp)
```{r}
tab_cp <- rpart::printcp(tree)
```

Here is a graphical visualization of CP vs error in cross validation
```{r}
rpart::plotcp(tree)
```
Identifying the best CP in cross-validation   
```{r}
tab_cp[which.min(tab_cp[,'xerror']),]
cp_min <- tab_cp[which.min(tab_cp[,'xerror']),'CP']
cp_min
```
Tuning tree  
```{r}
tree_tune <- rpart::rpart(preco ~ .,
                              data=cars_train2,
                              xval=0,
                              control = rpart.control(cp = cp_min, 
                                                      maxdepth = 30))
```

Predicted values
```{r}
cars_train2['p_tune'] = predict(tree_tune, cars_train2)
#cars_train2$p_tune %>% tail # investigar a previsão
cars_train2['r_tune'] = cars_train2$preco - cars_train2$p_tune
```

Evaluate the tuned tree
```{r}
metricas2(cars_train2$p_tune, cars_train2$preco)
```

Graphic analysis
```{r}
scatterplot_color(cars_train2, "p_tune", "preco", "r_tune")
#analise_grafica(cars_train2, "hodometro", "preco", "p_tune", "r_tune")
```
## Random Forest  

Dataset separation  
```{r}
set.seed(123)
separation <- sample(c("train", "test"),
                     size = nrow(cars_train),
                     replace = TRUE,
                     prob = c(0.8, 0.2))
```

Generating training and testing base
```{r}
train <- cars_train[separation == "train",]
nrow(train)
test <- cars_train[separation == "test",]
nrow(test)
```

```{r}
set.seed(123)
rf <- randomForest::randomForest(
  preco ~ .,
  data = train,
  ntree = 200
)
```
  
Evaluating tree
```{r}
pRF_train <- predict(rf, train)
pRF_test  <- predict(rf, test)

metricas2(pRF_train, train$preco)
metricas2(pRF_test, test$preco)
```
  
**The selected model will be Random forest, as it presented the best RMSE and R2 evaluation metrics.** 
 